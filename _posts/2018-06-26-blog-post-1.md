---
title: "Components of Reproducible Research"
author: Ben Buzzee
date: "May 21, 2018"
permalink: /posts/2018/06/blog-post-1/
fontsize: 14pt
output: 
  html_document
---


For the conclusions of a scientific study to be trustworthy, they must be verifiable. All too often, however, researchers never publish the data they used in their analysis, fail to adequately describe their methods, or use software that is inaccessible. The goal of the reproducible research movement is to conduct and document research in a manner that allows third parties to fully replicate the results of a study. This allows researchers not involved in the original study to completely understand the methods used and verify the correctness of any conclusions. This blog post will break down a reproducible research project that makes substantial use of data analysis or statistics into four major components. All components are independent of any particular type of software or statistical method.



## 1. The Data

Raw data is the starting point of any statistical analysis and its preservation is essential to verifying statistical results from beginning to end. Raw data may be the result of a database query, a spreadsheet where data was manually entered, or the raw results of a survey. It is the form of the data after the collection phase but before __any__ data processing or analysis has taken place.

The format of the data immediately after it has been collected is not always conducive to analysis. As such, the first task in a project involving data anyalsis is often converting the raw dataset into a "tidy" one. A "tidy" dataset is a dataset of a particular form that makes analysis and visualizations relatively easy and straightforward. According to Wickham (2014), a tidy dataset is one where:

* Each column corresponds to one variable
* Each row represents one observational unit
* Data from different types of observational units are stored in different data tables

When converting the raw data to a tidy dataset, it is important not to overwrite the original 'messy' dataset. Both the raw data and the clean data should be saved as separate files. Sometimes mistakes are made when converting the raw data to a useable format, and without the raw data, this cannot be checked for. Other researchers may also choose to process the raw data in a different manner, which requires the original raw data be kept intact.



## 2. The Code

The code used to conduct an analysis represents a complete description of all calculations and data manipulations. Without the code, a researcher would have to rely on general descriptions of calculations and will rarely know exactly how the analysis was performed. Code should always be saved and made available so researchers can re-create results locally on their own computer. For example, the script used to convert the raw data to a tidy dataset should be saved so that anyone can download the raw data and the data-tidying script, and then be able to recreate the tidy dataset from scratch on their own computer.



## 3. Documentation

A dataset without a description of what the variables represent or how the data were collected is useless to those not involved in its creation. Similarly, code that cannot be human-read or followed is of little help when it comes to understanding or verifying an analysis. Both the data and the code used must be well documented in order for a statistical analysis to be reproducible.

Data should always be accompanied by metadata. Metadata is a complete description of the data--the who, what, where, when, why, and how behind the data. The metadata file should allow parties not involved in the study or data collection process to fully understand what everything in the raw data file represents and how it got there. The metadata should fully explain--or provide a link to an explanation of--the sampling or experimental protocols used to collect the data. Creating and saving a metadata file with the dataset also makes it possible for the data to be reused for other purposes beyond what was originally intended, even after long periods of time have passed.

In addition to documenting the data, the code itself must be documented. Code should be well commented and written in a manner that is conducive to human reading. Custom functions should be accompanied by appropriate descriptions and instructions. Writing readable code makes the code less prone to error, easier to understand, easier to debug and improve, and makes collaboration a smoother process. A reader should be able to open a script without any prior knowledge of it and be able to attain a general understanding of what the code does and how it does it in a reasonably short amount of time. There are many guides to writing readable code online. One example for the R programming language can be found at http://adv-r.had.co.nz/Style.html.

For highly technical code, it may be better to write the code in an Rmarkdown or Jupyter Notebook type document. Rmarkdown and Jupyter Notebook documents allow code and output to be interwoven with well-formatted sections of text. This can significantly improve the readability of technical documents and allows for (and even encourages) explanations to be more thorough.



## 4. Organization and Accessibility

In order to be useful, all of the above components need to be organized and accessible. Ideally, any individual wishing to reproduce the results of a particular study should be able to quickly find the project in an archive or repository, and then immediately have access to--and be able to find--all relevant files.

Enabling others to find the files they're looking for in the first place is an underrated part of reproducible research. When dealing with computational projects, file clutter can slow down workflows and even render an entire project inaccessible. The problem of clutter can be solved by structuring a project directory in a clear and well-organized manner. The exact structure of a directory is arbitrary and will vary from project to project, but a few things should be common to all reproducible project directories: 1. the directory should contain everything needed to reproduce the analysis--including the data, code, documentation, and final product(s) 2. folders should be used to group files in a coherent way that makes finding files a straightforward process and 3. folders and files should be named in a manner that clearly indicates their contents. The following is an example of a simple directory for an R project. Additional project-specific folders can be added as needed. (/ indicates a folder) 

 * /Project
    * /Rscripts
    * /data
    * /reports
    * project.Rproj


Finally, to make a project accessible, it needs to be hosted somewhere that members of the relevant research community have access to. Options range from formal topic-specific archives dedicated to the long-term preservation of research to simple GitHub repositories. Regardless of where the project is stored, it needs to have an informative name, be tagged with appropriate keywords, and be searchable so that others can find it.



## Implementation

There are many types of software that can be used to piece together the above components. With time, specific types of software will inevitably change or become obsolete. In the long run, however, putting together code, data, and documentation in a coherent and accessible manner will remain a hallmark of high quality research.


# Sources & Reading Suggestions

Tidy data. Wickham 2014. https://www.jstatsoft.org/article/view/v059i10/

Packaging data analytical work reproducibly using R (and friends). Marwick 2018 https://peerj.com/preprints/3192/

Intro to GitHub Organizations:  https://blog.github.com/2010-06-29-introducing-organizations/
