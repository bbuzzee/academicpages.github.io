---
title: "The Fundemental Confidence Fallacy"
author: "Ben Buzzee"
permalink: /posts/2018/06/blog-post-2/
date: "June 26, 2018"
fontsize: 14pt
output: html_document
---

 "Claims that confidence intervals yield an
index of precision, that the values within them are plausible,
and that the confidence coefficient can be read as
a measure of certainty that the interval contains the true
value, are all fallacies and unjustified by confidence interval
theory." - The Fallacy of Placing Confidence in Confidence Intervals

Modern introductory statistics classes tend to focus exclusively on frequentist-style confidence intervals and hypothesis tests. As the field of statistics matures, many of the shortcomings of hypothesis tests have come to light, leaving the confidence interval as the preferred research tool for new and veteran practitioners of statistics alike.$^{1,2}$ Although taught in virtually every introductory statistics course in the United States, confidence intervals are widely misused and misinterpreted, even by professional scientists. $^3$ This post attempts to clarify exactly what confidence intervals are, how to correctly interpret them, and when it makes sense to use them.



# The Problem

The need for interval estimation arises when a researcher is interested in knowing the value of an important quantity that describes an entire population (a parameter), and, for one reason or another, the entire population cannot be measured. Thus the researcher must rely on a sample to estimate the value of the parameter. The problem, then, is that different samples typically produce different estimates of the parameter of interest. The solution is to create an interval that __somehow takes into account our uncertainty in a quantifiable manner.__ The best method for quantifying such uncertainty is the subject of much debate.

One solution, and the one taught in virtually every introductory statistics class in the United States, is the frequentist-style confidence interval. Confidence intervals were originally developed by Jerzy Neyman in his 1937 paper "Outline of a Theory of Statistical Estimation Based on the Classical Theory of Probability." This is the same Jerzy Neyman that worked with Egon Pearson earier in the 1930's to develop the theory behind frequentist-style hypothesis testing. $^4$ 

When it comes to confidence intervals, Neyman recommends a three step procedure: 1. collect a sample, 2. create the interval (using methodology laid out in the paper) and 3. claim the value of the parameter is in the interval.

He goes on to say (and try to make sense of this):

> The justification of this recommendation lies in the fact that the three steps
described are equivalent to a random experiment which may result either in a correct
or in an erroneous statement concerning the value of [the parameter], the probability of a correct
statement being equal to [our confidence level]. Thus the statistician following the above recommendation
is in a position comparable with that of a game of chance with the
probability of winning being equal to [our confidence level].$^5$

And so, according to Neyman, we do our study, create our interval, and claim the parameter is contained in the interval. The probability we "win," or that our claim is correct, is equal to our confidence level. And so the problem of coherently quantifying the uncertainty involved with estimating a parameter is solved... right? __Nope.__

Unfortunately, Neyman is a master of subtle convolution that leads to technically valid statements but a terribly misled reader. The first 10 times you read the above passage, unless you are a well trained statistician, you are (probably) not correctly interpreting the subtleties employed by Neyman.

The natural interpretation of the above passage--that we can create a confidence interval, make the claim that our interval contains the parameter, and be correct with X% probability is __NOT__ what the above passage says. We will get to the correct interpretation a bit later.

The issues with the above passage are a microcosm of all the problems with confidence intervals. They are difficult to (correctly) understand, it is natural and easy to misinterpret them, but they are technically valid mathematically. Further, it is often inconsequential to misinterpet them in the way we are naturally inclined to do. This has led to extremely widespread misuse and misinterpretation even by those who use confidence intervals professionally.


# Frequentist Probabilities

Confidence intervals are inextricably tied to the frequentist definition of probability. Frequentist style statistics defines the probability of an event as the relative frequency of that event as the number of trials (or times the event _could_ have occured) tends towards infinity. To make a probabilistic claim, it must be set up in the context of theoretical long-run frequencies of events. This is the definition typically used in an introductory statistics class--whether students (or even the teacher!) knows it.

For example, suppose we are interested in fair coin flips. The probability of a heads is equal to the limit of the ratio of flips that land heads-up to total flips, as the number of flips tends toward infinity. For a genuinely fair coin, this would be 1/2. Notice this is a theoretical probability--and no actual coin flipping was required! 

Now, say, we actually flipped a coin once. What is the probability the coin that now lays stationary on the ground is heads? Well, now, in __regards to that particular coin__ there are no trials to repeat. There is no randomness involved whatsoever with a coin that lays flat on the ground, and so our concept of frequentist probabilities simply __does not apply to this scenario__. 

The key takeaway here is that under the frequentist framework, __the probability of an event is inextricably tied to a process/trial that can be repeated indefinitely__. The probability of a fair coin landing on heads __only makes sense in the context of repeated coin flips.__


# The Confidence Interval

Now that we have an understanding of (frequentist-style) probability, we can define a confidence interval.


### Definition (Confidence Interval): 

> An X% confidence interval for a parameter theta is an interval (L,U) generated
by a procedure that in repeated sampling has an X% probability
of containing the true value of [the parameter] (Neyman, 1937).$^5$

This definition leads to some natural questions:

* Is the probability associated with __a procedure under repeated sampling__ scientifically relevant? 

* What is meant by repeated sampling? Two samples? Infinite samples? What about the all-important one sample we have in front of us?

* Do we have any level of certainty the parameter is in the interval?


### Back to the Quote:

Recall: Neymans recommendation is to 1. collect a sample, 2. create the interval (using his methodology) and 3. claim the value of the parameter is in the interval.

> The justification of this recommendation lies in the fact that the three steps
described are equivalent to a random experiment which may result either in a correct
or in an erroneous statement concerning the value of [the parameter], the probability of a correct
statement being equal to [our confidence level]. Thus the statistician following the above recommendation
is in a position comparable with that of a game of chance with the
probability of winning being equal to [our confidence level].


What Neyman is doing above, which we may not have noticed the first time we read his statement, is bundling all three steps of his recommendation into one __trial__. Thus when we talk about the probability of a correct statement (the claim that the interval brackets the parameter), we are talking about the limiting frequency of correct statements to total statements as the number of trials (one trial being the bundle of three steps) tends towards infinity. __Neyman is not making a claim about any one particular confidence interval, but rather the process of creating one.__

To digest this, let's revisit the coin analogy. We established that under the frequentist framework, the probability a fair coin lands on heads is 1/2. This is because the ratio of heads to total flips is 1/2 (for a fair coin) as the number of flips tends towards infinity. We also noted that after a single coin has been flipped, it makes no sense to talk about the probability of heads anymore. After the coin is flipped, there are no longer random trials we can find a limiting frequency from. we can take the limit of. 

Similarly, the probability an interval brackets the parameter is equal to our confidence level (usually .95). This is because, as determined by the procedure in step 2 of our Neyman's recommendations, the ratio of the number of correct statements to total statements is equal to our confidence level, as the total number of statements tends towards infinity. And just like in the coin tossing example, after the interval is created, there is no longer a random process we can take the limit of. __After an interval is created, we cannot apply a probabilistic framework to it--nothing is random anymore.__

### Natural Questions

___Is the probability associated with a procedure under repeated sampling scientifically relevant?___

So if we can't make probabilistic statements about particular realized intervals, how are they useful? Well, sometimes they aren't. But we can take a look at an example that illustrates when they might be useful:

Suppose we are fisheries managers and each year we create a 90% confidence interval for the total number of rainbow trout in a lake. Suppose 1000 dollars in fishing license revenue comes in from this lake. Also suppose, if we perform our three step confidence interval procedure and the total number if fish are outside the interval, we have to pay 5000 dollars to get the lake restocked.

__If we fix costs associated with our confidence interval procedure correctly bracketing the parameter, we can compute how much we can expect to lose on average__ In the above example, in the long run, we stand to make 1000 - 5000(.1) = 500 dollars (we earn the 1000 at the beginning of the year irregardless of our correctness). We could have to pay 5000 dollars two years in a row, but after enough years have passed (the coverage probability converges), we will find ourselves being correct close to 90% of the time. If, on average, you want to be correct about determing the value of a parameter, a confidence interval is useful. If you are the manager of a casino, or making yearly regulatory decisions with a government agency, a confidence interval will help you make the right decision a certain percentage of the time, on average.

When isn't a confidence interval appropriate? Any time the idea of repeated trials resulting in a random outcome doesn't make sense. Or when you only plan on doing something once. The idea may still be useful.



___What is meant by repeated sampling? Two samples? Infinite samples?___

We could be wrong with our first interval, and second interval, but how long until we are right approximately 95% of the time?


___Do we have any level of certainty the parameter is in the interval?___

No, but may be useful guiding force. Ex: Gameshow with curtain, going with long run probabilities might be best option.



# Conclusion

If, as part of your research, you are making the request: 

> "Give me an interval that will bracket the true value of the parameter in 100p% of the instances of an experiment that is repeated a large number of times."

Then a frequentist style confidence interval is the appropriate solution to that request. Arguably, however, that is not in-line with what many (most) researchers are seeking when it comes to estimating parameters.


>"The credible interval is an answer to the request: "Give me an interval that brackets the true value with probability p given the particular sample I've actually observed."




# SOURCES

1. When Null Hypothesis Significance Testing Is Unsuitable for Research: A Reassessment. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5540883/
2. The Insignificance of Statistical Significance Testing. https://www.jstor.org/stable/3802789?seq=1#page_scan_tab_contents
3. The Fallacy of Placing Confidence in Confidence Intervals. https://link.springer.com/article/10.3758/s13423-015-0947-8
4. Jerzy Neyman. http://magazine.amstat.org/blog/2017/04/13/sih-neyman/
5. Outline of a Theory of Statistical Estimation Based on the Classical Theory of Probability. Neyman. https://www.jstor.org/stable/91337?seq=1#page_scan_tab_contents




# SAVED STUFF

https://www.quora.com/How-can-I-deeply-understand-statistics

Quora 1:

> Here are some questions to develop deeper understanding of statistics (I may keep adding to this if I think of more):
 * Under what circumstances, if any, does it make sense to follow the convention of fixing probability of Type I error and maximizing power subject to that constraint?
 * Under what circumstances, if any, is a confidence interval a "reasonable range of values" for an unknown parameter?
 * Why was the word "confidence" introduced into statistical practice? What does it convey that "probability" does not?
 * Is it better to draw weak conclusions with weak assumptions (non-parametric methods) or strong conclusions with strong assumptions (Bayesian methods)?
  * Why are asymptotic results useful, given that real data is always finite?
  * What are the pros and cons of using an arbitrary p-value cutoff like 0.05 to decide whether research should be published?
  * If I analyze data from a clinical trial using a non-parametric test based on randomization of treatment assignments only, can I draw any conclusions about the effectiveness of the treatment in the full population? On what basis?
  * Under what circumstances should I do a one-tailed test versus a two-tailed test?
  * What, if anything, is so great about unbiasedness? -Michael Hochster



https://stats.stackexchange.com/questions/26450/why-does-a-95-confidence-interval-ci-not-imply-a-95-chance-of-containing-the/26457#26457
https://stats.stackexchange.com/questions/11609/clarification-on-interpreting-confidence-intervals

Quora 2:

> I think the fundamental problem is that frequentist statistics can only assign a probability to something that can have a long run frequency. Whether the true value of a parameter lies in a particular interval or not doesn't have a long run frequency, becuase we can only perform the experiment once, so you can't assign a frequentist probability to it. The problem arises from the definition of a probability. If you change the definition of a probability to a Bayesian one, then the problem instantly dissapears as you are no longer tied to discussion of long run frequencies.

> See my (rather tounge in cheek) answer to a related question here:

> "A Frequentist is someone that believes probabilies represent long run frequencies with which events ocurr; if needs be, he will invent a fictitious population from which your particular situation could be considered a random sample so that he can meaningfully talk about long run frequencies. If you ask him a question about a particular situation, he will not give a direct answer, but instead make a statement about this (possibly imaginary) population."

> In the case of a confidence interval, the question we normally would like to ask (unless we have a problem in quality control for example) is "given this sample of data, return the smallest interval that contains the true value of the parameter with probability X". However a frequentist can't do this as the experiment is only performed once and so there are no long run frequencies that can be used to assign a probability. So instead the frequentist has to invent a population of experiments (that you didn't perform) from which the experiment you did perform can be considered a random sample. The frequentist then gives you an indirect answer about that fictitious population of experiments, rather than a direct answer to the question you really wanted to ask about a particular experiment.
> Essentially it is a problem of language, the frequentist definition of a popuation simply doesn't allow discussion of the probability of the true value of a parameter lying in a particular interval. That doesn't mean frequentist statistics are bad, or not useful, but it is important to know the limitations.


Stat stack exchange:

>I think the premise of this question is flawed because it denies the distinction between the uncertain and the known.

> Describing a coin flip provides a good analogy. Before the coin is flipped, the outcome is uncertain; afterwards, it is no longer "hypothetical." Confusing this fait accompli with the actual situation we wish to understand (the behavior of the coin, or decisions that are to be made as a result of its outcome) essentially denies a role for probability in understanding the world.

> This contrast is thrown in sharp relief within an experimental or regulatory arena. In such cases the scientist or the regulator know they will be faced with situations whose outcomes, at any time beforehand, are unknown, yet they must make important determinations such as how to design the experiment or establish the criteria to use in determining compliance with regulations (for drug testing, workplace safety, environmental standards, and so on). These people and the institutions for which they work need methods and knowledge of the probabilistic characteristics of those methods in order to develop optimal and defensible strategies, such as good experimental designs and fair decision procedures that err as little as possible.

> Confidence intervals, despite their classically poor justification, fit into this decision-theoretic framework. When a method of constructing a random interval has a combination of good properties, such as assuring a minimal expected coverage of the interval and minimizing the expected length of the interval--both of them a priori properties, not a posteriori ones--then over a long career of using that method we can minimize the costs associated with the actions that are indicated by that method.



