---
title: "Statistics 101 (Work in Progress)"
author: "Ben Buzzee"
permalink: /posts/2018/06/blog-post-2/
date: "June 26, 2018"
output: html_document
---

Modern introductory statistics classes tend to focus exclusively on frequentist style confidence intervals and hypothesis tests. As many new (and veteran) students of statistics know, these methods can be confusing and difficult to interpret. This post is an attempt to piece together a better picture of very basic statistical ideas--purely in terms of conceptual cogency.

Motivation:

 * "The reason students have problems understanding hypothesis
tests is that they may be trying to think." - The Insignificance of Statistical Significance Testing

 * "Example number one: the confidence interval. Its definition is so con-
trived and anti-intuitive that it's no wonder that students have difficulties
ingesting it." - William Briggs

 * "Claims that confidence intervals yield an
index of precision, that the values within them are plausible,
and that the confidence coefficient can be read as
a measure of certainty that the interval contains the true
value, are all fallacies and unjustified by confidence interval
theory." - The fallacy of placing confidence in confidence intervals


## Frequentist Methods

### Confidence Intervals 

Context: When the goal is to estimate the value of a parameter (an unknowable constant of interest) and get a sense of the variability of the estimator, the gold standard for frequentists is the confidence interval. 

Definition (Confidence Interval): 
> An X% confidence interval for a parameter theta is an interval (L,U) generated
by a procedure that in repeated sampling has an X% probability
of containing the true value of theta (Neyman, 1937).

Interpretation: Countless students have been taught to memorize the following interpretation: "We are 95% confident the parameter is between L and U." But this statement is ambiguous. What does "confident" mean? When inexperienced practitioners of statistics encounter this ambiguity, it is only natural for them to  relate confidence to probabilities.

A great analogy: You have a rubber ball and a large plastic bowl. The ball represents a parameter and the bowl represents the confidence region. The natural (and incorrect) interpretation is to state that when tossing the ball, there is a 95% chance it lands in the bowl. The correct interpretation requires the ball be fixed on the ground, and if we tossed the bowl an infinite number of times, 95% of those tosses would cover the ball. Parameters are fixed and non-random. The bounds on the interval are random due to the sampling process.

An interval __generated by a procedure that__ as the number of samples tends to infinity has a 95% probability of containing the true value of the parameter. So the procedure has the probability, and only when the numnber of samples increases towards infinite. We cannot make any probabilistic claims about the one interval we created.

What are confidence intervals good for? 

Flaws: Interpretability, probability is about interval--not parameter, does not provide a sense of variability of the estimator


### Hypothesis Tests 

Context: When the goal is use an arbitrary level of "rareness under the null" to make a binary decision about a statistical hypothesis.

Flaws: Says nothing about effect size, anything can be "significant,"" significance level is arbitrary, fails to account for a gradient of evidence.


## Bayesian alternatives


### Credible Intervals

### Scratch the Tests!

## Non-Bayesian Alternatives

Likelihood methods


Logical Foundations: How does statistical evidence lead to understanding "truth" in nature? Popperian style falsification? Are hypothesis tests useful or are they misleading? "Accepting the Alternative?" A statistical procedure needs to be paired with a logical framework in order to draw conclusions about the natural world.

Statistical Evidence: P-values, effect size, likelihood ratios, posterior probabilities




