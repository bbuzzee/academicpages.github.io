---
title: "Statistics 101 (Work in Progress)"
author: "Ben Buzzee"
permalink: /posts/2018/06/blog-post-2/
date: "June 26, 2018"
output: html_document
---

Modern introductory statistics classes tend to focus exclusively on frequentist style confidence intervals and hypothesis tests. As many new (and veteran) students of statistics know, these methods can be confusing and difficult to interpret. This post is an attempt to piece together a better picture of very basic statistical ideas--purely in terms of conceptual cogency.

Motivation:

 * "The reason students have problems understanding hypothesis
tests is that they may be trying to think." - The Insignificance of Statistical Significance Testing

 * "Example number one: the confidence interval. Its definition is so con-
trived and anti-intuitive that it's no wonder that students have difficulties
ingesting it." - William Briggs

 * "Claims that confidence intervals yield an
index of precision, that the values within them are plausible,
and that the confidence coefficient can be read as
a measure of certainty that the interval contains the true
value, are all fallacies and unjustified by confidence interval
theory." - The fallacy of placing confidence in confidence intervals


## Frequentist Methods

### Confidence Intervals 

What are confidence intervals?

Definition (Confidence Interval): 
> An X% confidence interval for a parameter theta is an interval (L,U) generated
by a procedure that in repeated sampling has an X% probability
of containing the true value of theta (Neyman, 1937).


A great analogy: You have a rubber ball and a large plastic bowl. The ball represents a parameter and the bowl represents the confidence region. The natural (and incorrect) interpretation is to state that when tossing the ball, there is a 95% chance it lands in the bowl. The correct interpretation requires the ball be fixed on the ground, and if we toss the bowl repeatedly, the percentage of time the bowl lands on the ball tends towards 95% as the number of tosses increases. But what does "tends towards" mean, and does it matter?

An interval _generated by a procedure that_ as the number of samples tends to infinity has a 95% probability of containing the true value of the parameter. So the probability is associated with the procedure, and this probability _tends_ towards the confidence level as the sampling process is repeated.  We cannot make any probabilistic claims about the one interval we created, only the process under the context of repeated sampling.


What are confidence intervals good for? Can we learn any more than what is provided by the definition?

Do they:
* provide a sense of variability of the estimate?
* provide a range of plausible values?



### Problems with confidence intervals
 * Misinterpretation & confusion
 * Answering the wrong question?







### Hypothesis Tests 

Context: When the goal is use an arbitrary level of "rareness under the null" to make a binary decision about a statistical hypothesis.

Flaws: Says nothing about effect size, anything can be "significant,"" significance level is arbitrary, fails to account for a gradient of evidence.


## Bayesian alternatives


### Credible Intervals

### Scratch the Tests!

## Non-Bayesian Alternatives

Likelihood methods


Logical Foundations: How does statistical evidence lead to understanding "truth" in nature? Popperian style falsification? Are hypothesis tests useful or are they misleading? "Accepting the Alternative?" A statistical procedure needs to be paired with a logical framework in order to draw conclusions about the natural world.

Statistical Evidence: P-values, effect size, likelihood ratios, posterior probabilities


> Here are some questions to develop deeper understanding of statistics (I may keep adding to this if I think of more):
 * Under what circumstances, if any, does it make sense to follow the convention of fixing probability of Type I error and maximizing power subject to that constraint?
 * Under what circumstances, if any, is a confidence interval a "reasonable range of values" for an unknown parameter?
 * Why was the word "confidence" introduced into statistical practice? What does it convey that "probability" does not?
 * Is it better to draw weak conclusions with weak assumptions (non-parametric methods) or strong conclusions with strong assumptions (Bayesian methods)?
  * Why are asymptotic results useful, given that real data is always finite?
  * What are the pros and cons of using an arbitrary p-value cutoff like 0.05 to decide whether research should be published?
  * If I analyze data from a clinical trial using a non-parametric test based on randomization of treatment assignments only, can I draw any conclusions about the effectiveness of the treatment in the full population? On what basis?
  * Under what circumstances should I do a one-tailed test versus a two-tailed test?
  * What, if anything, is so great about unbiasedness? -Michael Hochster


